<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Jon Nordby jon@soundsensing.no">
  <meta name="dcterms.date" content="2019-11-19">
  <title>Sensecamp2019: Classification of Environmental Sound using IoT sensors</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reset.css">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">
  <link rel="stylesheet" href="style.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Classification of Environmental Sound using IoT sensors</h1>
  <p class="author">Jon Nordby <a href="mailto:jon@soundsensing.no" class="email">jon@soundsensing.no</a></p>
  <p class="date">November 19, 2019</p>
</section>

<section><section id="introduction" class="title-slide slide level1"><h1>Introduction</h1></section><section id="jon-nordby" class="slide level2">
<h2>Jon Nordby</h2>
<p>Internet of Things specialist</p>
<ul>
<li>B.Eng in <strong>Electronics</strong></li>
<li>9 years as <strong>Software</strong> developer. <strong>Embedded</strong> + <strong>Web</strong></li>
<li>M. Sc in <strong>Data</strong> Science</li>
</ul>
<p>Now:</p>
<ul>
<li>CTO at Soundsensing</li>
<li>Machine Learning Consultant</li>
</ul>
</section><section id="soundsensing" class="slide level2">
<h2>Soundsensing</h2>
<p><img data-src="./img/soundsensing-withlogo.png" style="width:100.0%" /></p>
<aside class="notes">
<p>Provide <strong>Noise Monitoring</strong> and Audio <strong>Condition Monitoring</strong> solutions that are used in Real-Estate, Industry, and Smart Cities.</p>
<p>Perform Machine Learning for sound classification <strong>on sensor</strong>.</p>
</aside>
</section><section id="dashboard" class="slide level2">
<h2>Dashboard</h2>
<figure>
<img data-src="img/what-we-do.png" alt="" /><figcaption>Pilot projects with customers Now - 2020</figcaption>
</figure>
</section><section id="thesis" class="slide level2">
<h2>Thesis</h2>
<blockquote>
<p>Environmental Sound Classification on Microcontrollers using Convolutional Neural Networks</p>
</blockquote>
<figure>
<img data-src="./img/thesis.png" style="width:30.0%" alt="" /><figcaption>Report &amp; Code: https://github.com/jonnor/ESC-CNN-microcontroller</figcaption>
</figure>
</section><section id="wireless-sensor-networks" class="slide level2">
<h2>Wireless Sensor Networks</h2>
<ul>
<li>Want: Wide and dense coverage</li>
<li>Need: Sensors need to be low-cost</li>
<li><strong>Opportunity</strong>: Wireless reduces costs</li>
<li><strong>Challenge</strong>: Power consumption</li>
</ul>
<aside class="notes">
<ul>
<li>No network cabling, no power cabling</li>
<li>No site infrastructure needed</li>
<li>Less invasive</li>
<li>Fewer approvals needed</li>
<li>Temporary installs feasible</li>
<li>Mobile sensors possible</li>
</ul>
<p>Electrician is 750 NOK/hour</p>
<p>Image: https://www.nti-audio.com/en/applications/noise-measurement/unattended-monitoring</p>
</aside>
</section><section id="sensor-network-architectures" class="slide level2">
<h2>Sensor Network Architectures</h2>
<p><img data-src="img/sensornetworks.png" style="width:70.0%" /></p>
</section></section>
<section><section id="audio-machine-learning-on-low-power-sensors" class="title-slide slide level1"><h1>Audio Machine Learning on low-power sensors</h1></section><section id="what-do-you-mean-by-low-power" class="slide level2">
<h2>What do you mean by low-power?</h2>
<p>Want: 1 year lifetime for palm-sized battery</p>
<p>Need: <code>&lt;1mW</code> system power</p>
</section><section id="general-purpose-microcontroller" class="slide level2">
<h2>General purpose microcontroller</h2>
<p><img data-src="img/cortexM4.png" style="width:40.0%" /></p>
<p>STM32L4 @ 80 MHz. Approx <strong>10 mW</strong>.</p>
<ul>
<li>TensorFlow Lite for Microcontrollers (Google)</li>
<li>ST X-CUBE-AI (ST Microelectronics)</li>
</ul>
</section><section id="fpga" class="slide level2">
<h2>FPGA</h2>
<figure>
<img data-src="img/iCE40UltraPlus.png" style="width:50.0%" alt="" /><figcaption>Lattice ICE40 UltraPlus with Lattice sensAI</figcaption>
</figure>
<p>Human presence detection. VGG8 on 64x64 RGB image, 5 FPS: 7 mW.</p>
<p>Audio ML approx <strong>1 mW</strong></p>
</section><section id="neural-network-co-processors" class="slide level2">
<h2>Neural Network co-processors</h2>
<figure>
<img data-src="img/ST-Orlando-SoC.png" style="width:25.0%" alt="" /><figcaption>Project Orlando (ST Microelectronics), expected 2020</figcaption>
</figure>
<p>2.9 TOPS/W. AlexNet, 1000 classes, 10 FPS. 41 mWatt</p>
<p>Audio models probably <strong>&lt; 1 mWatt</strong>.</p>
<aside class="notes">
<p>https://www.latticesemi.com/Blog/2019/05/17/18/25/sensAI</p>
</aside>
</section></section>
<section><section id="on-edge-classification-of-noise" class="title-slide slide level1"><h1>On-edge Classification of Noise</h1></section><section id="environmental-sound-classification" class="slide level2">
<h2>Environmental Sound Classification</h2>
<blockquote>
<p>Given an audio signal of environmental sounds,</p>
<p>determine which class it belongs to</p>
</blockquote>
<ul>
<li>Widely researched. 1000 hits on Google Scholar</li>
<li>Datasets. Urbansound8k (10 classes), ESC-50, AudioSet (632 classes)</li>
<li>2017: Human-level performance on ESC-50</li>
</ul>
<aside class="notes">
<p>https://github.com/karoldvl/ESC-50</p>
</aside>
<!--

## Microcontroller

![](img/sensortile-annotated.jpg){width=100%}

-->
<aside class="notes">
<p>STM32L476</p>
<p>ARM Cortex M4F Hardware floating-point unit (FPU) DSP SIMD instructions 80 MHz CPU clock 1024 kB of program memory (Flash) 128 kB of RAM.</p>
<p>25 mWatt max</p>
</aside>
</section><section id="urbansound8k" class="slide level2">
<h2>Urbansound8k</h2>
<p><img data-src="img/urbansound8k-examples.png" style="width:100.0%" /></p>
<aside class="notes">
<p>Classes from an urban sound taxonomy, based on noise complains in New York city</p>
<p>Most sounds around 4 seconds. Some classes around 1 second</p>
<p>Foreground/background</p>
</aside>
</section><section id="existing-work" class="slide level2">
<h2>Existing work</h2>
<ul>
<li>Convolutional Neural Networks dominate</li>
<li>Techniques come from image classification</li>
<li>Mel-spectrogram input standard</li>
<li>End2end models: getting close in accuracy</li>
<li>“Edge ML” focused on mobile-phone class HW</li>
<li>“Tiny ML” (sensors) just starting</li>
</ul>
<aside class="notes">
<ul>
<li>Efficient Keyword-Spotting</li>
<li>Efficient (image) CNNs</li>
<li>Efficient ESC-CNN</li>
</ul>
<p>ESC-CNN</p>
<ul>
<li>23 papers reviewed in detail</li>
<li>10 referenced in thesis</li>
<li>Only 4 consider computational efficiency</li>
</ul>
</aside>
</section><section id="model-requirements" class="slide level2">
<h2>Model requirements</h2>
<p>With 50% of STM32L476 capacity:</p>
<ul>
<li>64 kB RAM</li>
<li>512 kB FLASH memory</li>
<li>4.5 M MACC/second</li>
</ul>
<aside class="notes">
<ul>
<li>RAM: 1000x 64 MB</li>
<li>PROGMEM: 1000x 512 MB</li>
<li>CPU: 1000x 5 GFLOPS</li>
<li>GPU: 1000’000X 5 TFLOPS</li>
</ul>
</aside>
</section><section id="existing-models" class="slide level2">
<h2>Existing models</h2>
<figure>
<img data-src="img/urbansound8k-existing-models-logmel.png" style="width:100.0%" alt="" /><figcaption>Green: Feasible region</figcaption>
</figure>
<p>eGRU: running on ARM Cortex-M0 microcontroller, accuracy 61% with <strong>non-standard</strong> evaluation</p>
<aside class="notes">
<p>Assuming no overlap. Most models use very high overlap, 100X higher compute</p>
</aside>
</section><section id="pipeline" class="slide level2">
<h2>Pipeline</h2>
<p><img data-src="img/classification-pipeline.png" data-max-height="100%" /></p>
</section><section id="models" class="slide level2">
<h2>Models</h2>
<!--
Based on SB-CNN (Salamon+Bello, 2016)
-->
<p><img data-src="img/models.svg" style="width:70.0%" /></p>
<aside class="notes">
<p>Baseline from SB-CNN</p>
<p>Few modifications</p>
<ul>
<li>Uses smaller input feature representation</li>
<li>Reduced downsample factor to accommodate</li>
</ul>
<p>CONV = entry point for trying different convolution operators</p>
</aside>
</section></section>
<section><section id="strategies-for-shrinking-convolutional-neural-network" class="title-slide slide level1"><h1>Strategies for shrinking Convolutional Neural Network</h1></section><section id="reduce-input-dimensionality" class="slide level2">
<h2>Reduce input dimensionality</h2>
<p><img data-src="img/input-size.svg" style="width:70.0%" /></p>
<ul>
<li>Lower frequency range</li>
<li>Lower frequency resolution</li>
<li>Lower time duration in window</li>
<li>Lower time resolution</li>
</ul>
<aside class="notes">
<p>Directly limits time and RAM use first few layers.</p>
<p>Follow-on effects. A simpler input representation is (hopefully) easier to learn allowing for a simpler model</p>
<p>TODO: make a picture illustrating this</p>
</aside>
</section><section id="reduce-overlap" class="slide level2">
<h2>Reduce overlap</h2>
<p><img data-src="img/framing.png" style="width:80.0%" /></p>
<p>Models in literature use 95% overlap or more. 20x penalty in inference time!</p>
<p>Often low performance benefit. Use 0% (1x) or 50% (2x).</p>
<!--
## Regular 2D-convolution

![](img/convolution-2d.png){width=100%}

::: notes

TODO: illustrate the cubical nature. Many channel

:::
-->
</section><section id="depthwise-separable-convolution" class="slide level2">
<h2>Depthwise-separable Convolution</h2>
<p><img data-src="img/depthwise-separable-convolution.png" style="width:90.0%" /></p>
<p>MobileNet, “Hello Edge”, AclNet. 3x3 kernel,64 filters: 7.5x speedup</p>
<aside class="notes">
<ul>
<li>Much fewer operations</li>
<li>Less expressive - but regularization effect can be beneficial</li>
</ul>
</aside>
</section><section id="spatially-separable-convolution" class="slide level2">
<h2>Spatially-separable Convolution</h2>
<p><img data-src="img/spatially-separable-convolution.png" style="width:90.0%" /></p>
<p>EffNet, LD-CNN. 5x5 kernel: 2.5x speedup</p>
</section><section id="downsampling-using-max-pooling" class="slide level2">
<h2>Downsampling using max-pooling</h2>
<p><img data-src="img/maxpooling.png" style="width:100.0%" /></p>
<p>Wasteful? Computing convolutions, then throwing away 3/4 of results!</p>
</section><section id="downsampling-using-strided-convolution" class="slide level2">
<h2>Downsampling using strided convolution</h2>
<p><img data-src="img/strided-convolution.png" style="width:100.0%" /></p>
<p>Striding means fewer computations and “learned” downsampling</p>
</section><section id="model-comparison" class="slide level2">
<h2>Model comparison</h2>
<p><img data-src="img/models_accuracy.png" style="width:100.0%" /></p>
<aside class="notes">
<ul>
<li>Baseline relative to SB-CNN and LD-CNN is down from 79% to 73% Expected because poorer input representation. Much lower overlap</li>
</ul>
</aside>
</section><section id="performance-vs-compute" class="slide level2">
<h2>Performance vs compute</h2>
<p><img data-src="img/models_efficiency.png" style="width:100.0%" /></p>
<p>:::</p>
<ul>
<li>Performance of Strided-DS-24 similar to Baseline despite 12x the CPU use</li>
<li>Suprising? Stride alone worse than Strided-DS-24</li>
<li>Bottleneck and EffNet performed poorly</li>
<li>Practical speedup not linear with MACC</li>
</ul>
<p>:::</p>
<!--

-->
</section><section id="quantization" class="slide level2">
<h2>Quantization</h2>
<p>Inference can often use 8 bit integers instead of 32 bit floats</p>
<ul>
<li>1/4 the size for weights (FLASH) and activations (RAM)</li>
<li>8bit <strong>SIMD</strong> on ARM Cortex M4F: 1/4 the inference time</li>
<li>Supported in X-CUBE-AI 4.x (July 2019)</li>
</ul>
<!--

## Stronger training process 

- Data Augmentation. *Mixup*, *SpecAugment*
- Transfer Learning on more data. *AudioSet*

-->
<aside class="notes">
<p>EnvNet-v2 got 78.3% on Urbansound8k with 16 kHz</p>
</aside>
</section><section id="conclusions" class="slide level2">
<h2>Conclusions</h2>
<ul>
<li>Able to perform Environmental Sound Classification at <code>~ 10mW</code> power,</li>
<li>Using <em>general purpose microcontroller</em>, ARM Cortex M4F</li>
<li>Best performance: 70.9% mean accuracy, under 20% CPU load</li>
<li>Highest reported Urbansound8k on microcontroller (over eGRU 62%)</li>
<li>Best architecture: Depthwise-Separable convolutions with striding</li>
<li>Quantization enables 4x bigger models (and higher perf)</li>
<li>With dedicated Neural Network Hardware</li>
</ul>
</section></section>
<section><section id="further-research" class="title-slide slide level1"><h1>Further Research</h1></section><section id="waveform-input-to-model" class="slide level2">
<h2>Waveform input to model</h2>
<ul>
<li>Preprocessing. Mel-spectrogram: <strong>60</strong> milliseconds</li>
<li>CNN. Stride-DS-24: <strong>81</strong> milliseconds</li>
<li>With quantization, spectrogram conversion is the bottleneck!</li>
<li>Convolutions can be used to learn a Time-Frequency transformation.</li>
</ul>
<p>Can this be faster than the standard FFT? And still perform well?</p>
<aside class="notes">
<ul>
<li>Especially interesting with CNN hardware acceleration.</li>
</ul>
</aside>
</section><section id="on-sensor-inference-challenges" class="slide level2">
<h2>On-sensor inference challenges</h2>
<ul>
<li>Reducing power consumption. Adaptive sampling</li>
<li>Efficient training data collection in WSN. Active Learning?</li>
<li>Real-life performance evaluations. Out-of-domain samples</li>
</ul>
<aside class="notes">
<p>TODO: Add few more projects here. From research document</p>
</aside>
</section></section>
<section><section id="wrapping-up" class="title-slide slide level1"><h1>Wrapping up</h1></section><section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li>Noise pollution is a growing problem</li>
<li>Wireless Sensor Networks can used to quantify</li>
<li>Noise Classification can provide more information</li>
<li>Want high density of sensors. Need to be low cost</li>
<li>On-sensor classification desirable for power/cost and privacy</li>
</ul>
</section><section id="more-resources" class="slide level2">
<h2>More resources</h2>
<p>Machine Hearing. ML on Audio</p>
<ul>
<li><a href="https://github.com/jonnor/machinehearing">github.com/jonnor/machinehearing</a></li>
</ul>
<p>Machine Learning for Embedded / IoT</p>
<ul>
<li><a href="https://github.com/jonnor/embeddedml">github.com/jonnor/embeddedml</a></li>
</ul>
<p>Thesis Report &amp; Code</p>
<ul>
<li><a href="https://github.com/jonnor/ESC-CNN-microcontroller">github.com/jonnor/ESC-CNN-microcontroller</a></li>
</ul>
</section><section id="questions" class="slide level2">
<h2>Questions</h2>
<h1 style="padding: 100px">
?
</h1>
<p>Email: <a href="mailto:jon@soundsensing.no" class="email">jon@soundsensing.no</a></p>
</section><section id="come-talk-to-me" class="slide level2">
<h2>Come talk to me!</h2>
<ul>
<li>Noise Monitoring sensors. Pilot projects for 2020?</li>
<li>Environmental Sound, Wireless Sensor Networks for Audio. Research partnering?</li>
<li>“On-edge” / Embedded Device ML. Happy to advise!</li>
</ul>
<p>Email: <a href="mailto:jon@soundsensing.no" class="email">jon@soundsensing.no</a></p>
</section></section>
<section><section id="thesis-results" class="title-slide slide level1"><h1>Thesis results</h1></section><section id="model-comparison-1" class="slide level2">
<h2>Model comparison</h2>
<p><img data-src="img/models_accuracy.png" style="width:100.0%" /></p>
<aside class="notes">
<ul>
<li>Baseline relative to SB-CNN and LD-CNN is down from 79% to 73% Expected because poorer input representation. Much lower overlap</li>
</ul>
</aside>
</section><section id="list-of-results" class="slide level2">
<h2>List of results</h2>
<p><img data-src="img/results.png" style="width:100.0%" /></p>
</section><section id="confusion" class="slide level2">
<h2>Confusion</h2>
<p><img data-src="img/confusion_test.png" style="width:70.0%" /></p>
</section><section id="grouped-classification" class="slide level2">
<h2>Grouped classification</h2>
<p><img data-src="img/grouped_confusion_test_foreground.png" style="width:60.0%" /></p>
<p>Foreground-only</p>
</section><section id="unknown-class" class="slide level2">
<h2>Unknown class</h2>
<p><img data-src="img/unknown-class.png" style="width:100.0%" /></p>
<aside class="notes">
<p>Idea: If confidence of model is low, consider it as “unknown”</p>
<ul>
<li>Left: Histogram of correct/incorrect predictions</li>
<li>Right: Precision/recall curves</li>
<li>Precision improves at expense of recall</li>
<li>90%+ precision possible at 40% recall</li>
</ul>
<p>Usefulness:</p>
<ul>
<li>Avoids making decisions on poor grounds</li>
<li>“Unknown” samples good candidates for labeling-&gt;dataset. Active Learning</li>
<li>Low recall not a problem? Data is abundant, 15 samples a 4 seconds per minute per sensor</li>
</ul>
</aside>
</section></section>
<section><section id="experimental-details" class="title-slide slide level1"><h1>Experimental Details</h1></section><section id="all-models" class="slide level2">
<h2>All models</h2>
<p><img data-src="img/models-list.png" /></p>
<aside class="notes">
<ul>
<li>Baseline is outside requirements</li>
<li>Rest fits the theoretical constraints</li>
<li>Sometimes had to reduce number of base filters to 22 to fit in RAM</li>
</ul>
</aside>
</section></section>
<section><section id="methods" class="title-slide slide level1"><h1>Methods</h1><p>Standard procedure for Urbansound8k</p>
<ul>
<li>Classification problem</li>
<li>4 second sound clips</li>
<li>10 classes</li>
<li>10-fold cross-validation, predefined</li>
<li>Metric: Accuracy</li>
</ul></section><section id="training-settings" class="slide level2">
<h2>Training settings</h2>
<p><img data-src="img/training-settings.png" /></p>
</section><section id="training" class="slide level2">
<h2>Training</h2>
<ul>
<li>NVidia RTX2060 GPU 6 GB</li>
<li>10 models x 10 folds = 100 training jobs</li>
<li>100 epochs</li>
<li>3 jobs in parallel</li>
<li>36 hours total</li>
</ul>
<aside class="notes">
<ul>
<li>! GPU utilization only 15%</li>
<li>CPU utilization was near 100%</li>
<li>Larger models to utilize GPU better?</li>
<li>Parallel processing limited by RAM of biggest models</li>
<li>GPU-based augmentation might be faster</li>
</ul>
</aside>
</section><section id="evaluation" class="slide level2">
<h2>Evaluation</h2>
<p>For each fold of each model</p>
<ol type="1">
<li>Select best model based on validation accuracy</li>
<li>Calculate accuracy on test set</li>
</ol>
<p>For each model</p>
<ul>
<li>Measure CPU time on device</li>
</ul>
</section></section>
<section><section id="your-model-will-trick-you" class="title-slide slide level1"><h1>Your model will trick you</h1><p>And the bugs can be hard to spot</p></section><section id="fail-integer-truncation" class="slide level2">
<h2>FAIL: Integer truncation</h2>
<p><img data-src="img/fail-truncation.png" style="width:100.0%" /></p>
</section><section id="fail.-dropout-location" class="slide level2">
<h2>FAIL. Dropout location</h2>
<p><img data-src="img/fail-dropout.png" style="width:100.0%" /></p>
</section></section>
<section><section id="background" class="title-slide slide level1"><h1>Background</h1></section><section id="mel-spectrogram" class="slide level2">
<h2>Mel-spectrogram</h2>
<p><img data-src="img/spectrograms.svg" /></p>
</section><section id="noise-pollution" class="slide level2">
<h2>Noise Pollution</h2>
<p>Reduces health due to stress and loss of sleep</p>
<p>In Norway</p>
<ul>
<li>1.9 million affected by road noise (2014, SSB)</li>
<li>10’000 healty years lost per year (Folkehelseinstituttet)</li>
</ul>
<p>In Europe</p>
<ul>
<li>13 million suffering from sleep disturbance (EEA)</li>
<li>900’000 DALY lost (WHO)</li>
</ul>
<aside class="notes">
<p>1.9 million https://www.ssb.no/natur-og-miljo/artikler-og-publikasjoner/flere-nordmenn-utsatt-for-stoy</p>
<p>1999: 1.2 million</p>
<p>10 245 tapte friske leveår i Norge hvert år https://www.miljostatus.no/tema/stoy/stoy-og-helse/</p>
<p>https://www.eea.europa.eu/themes/human/noise/noise-2</p>
<p>Burden of Disease WHO http://www.euro.who.int/__data/assets/pdf_file/0008/136466/e94888.pdf</p>
</aside>
</section><section id="noise-mapping" class="slide level2">
<h2>Noise Mapping</h2>
<p>Simulation only, no direct measurements</p>
<p><img data-src="img/stoykart.png" /></p>
<aside class="notes">
<ul>
<li>Known sources</li>
<li>Yearly average value</li>
<li>Updated every 5 years</li>
<li>Low data quality. Ex: communal roads</li>
</ul>
<p>Image: https://www.regjeringen.no/no/tema/plan-bygg-og-eiendom/plan–og-bygningsloven/plan/kunnskapsgrunnlaget-i-planlegging/statistikk-i-plan/id2396747/</p>
</aside>
</section></section>
    </div>
  </div>

  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,
        height: 1080,
        // Factor of the display size that should remain empty around the content
        margin: 0,

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
